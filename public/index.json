[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m an avid applied researcher, who loves playing at the intersection of signal processing, machine learning, and software engineering. I currently work at Pindrop to design and build audio-based biometrics and fraud detection solutions.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://anthonyagnone.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m an avid applied researcher, who loves playing at the intersection of signal processing, machine learning, and software engineering. I currently work at Pindrop to design and build audio-based biometrics and fraud detection solutions.","tags":null,"title":"Anthony Agnone","type":"authors"},{"authors":["anthonyagnone"],"categories":["data"],"content":" Wait, but\u0026nbsp;Why? I’m in the process of closing on my first home in Atlanta, GA, and have been heavily using various real estate websites like Zillow, Redfin, and Trulia. I’ve also been toying with Zillow’s API, although somewhat spotty in functionality and documentation. Despite its shortcomings, I was fully inspired once I read the post by Lukas Frei on using the folium library to seamlessly create geography-based visualizations. A few days and some quick fun later, I’ve combined Zillow and Folium to make some cool visualizations of housing prices both within Atlanta and across the U.S.\nTopics  API integration Graph traversal Visualization  A Small Working\u0026nbsp;Example Let’s start simple by using some pre-aggregated data I downloaded from the Zillow website. This data set shows the median price by square foot for every state in the U.S. for each month from April 1996 to May 2019. Naturally, one could build a rich visualization on the progression of these prices over time; however, let’s stick with the most recent prices for now, which are in the last column of the file.\nHaving a look at the top-10 states, there aren’t many surprises. To be clear, I was initially caught off guard by the ordering of some of these, notably D.C. and Hawaii topping the chart. However, recall the normalization of “per square foot” in the metric. By that token, I’m maybe more surprised now that California still hits #3, given its size.\nTop 10 price/sqft in thousands of $$$ (May\u0026nbsp;2019)  Anyways, onto the show! Since this is a visualization article, I’ll avoid throwing too many lines of code in your face, and link it all to you to it at the end of the article. In short, I downloaded a GeoJSON file of the U.S. states from the folium repo. This was a great find, because it immediately gave me the schema of the data that I needed to give to folium for a seamless process; the only information I needed to add was the pricing data (to generate coloring in the final map). After providing that, a mere 5 lines of code got me the following plot:\nHeatmap of price/sqft of homes in the U.S. for May\u0026nbsp;2019  One Step\u0026nbsp;Further Now that I’d dipped my toes into the waters of Zillow and Folium, I was ready to be immersed. I decided to create a heat map of Metro Atlanta housing prices. One of the drawbacks of the Zillow API is that it’s rather limited in search functionality — I couldn’t find any way to perform a search based on lat/long coordinates, which would have been quite convenient for creating a granular heat map. Nevertheless, I took it as an opportunity to brush up on some crawler-style code; I used the results of an initial search by a city’s name as seeds for future calls to get the comps (via the GetComps endpoint) of those homes.\nIt’s worth noting that Zillow does have plenty of URL-based search filters that one could use to e.g. search by lat/long (see below). Obtaining the homes from the web page then becomes a scraping job, though, and you are subject to any sudden changes in Zillow’s web page structure. That being said, scraping projects can be a lot of fun; if you’d like to build this into what I made, let me know!\n# an example of a Zillow search URL, with plenty of specifications\nhttps://www.zillow.com/atlanta-ga/houses/2-_beds/2.0-_baths/?searchQueryState={%22pagination%22:{},%22mapBounds%22:{%22west%22:-84.88217862207034,%22east%22:-84.07880337792972,%22south%22:33.53377471775447,%22north%22:33.999556422130006},%22usersSearchTerm%22:%22Atlanta,%20GA%22,%22regionSelection%22:[{%22regionId%22:37211,%22regionType%22:6}],%22isMapVisible%22:true,%22mapZoom%22:11,%22filterState%22:{%22price%22:{%22min%22:300000,%22max%22:600000},%22monthlyPayment%22:{%22min%22:1119,%22max%22:2237},%22hoa%22:{%22max%22:200},%22beds%22:{%22min%22:2},%22baths%22:{%22min%22:2},%22sqft%22:{%22min%22:1300},%22isAuction%22:{%22value%22:false},%22isMakeMeMove%22:{%22value%22:false},%22isMultiFamily%22:{%22value%22:false},%22isManufactured%22:{%22value%22:false},%22isLotLand%22:{%22value%22:false},%22isPreMarketForeclosure%22:{%22value%22:false},%22isPreMarketPreForeclosure%22:{%22value%22:false}},%22isListVisible%22:true} Returning to the chosen path, I mentioned that I used initial results as entry points into the web of homes in a given city. With those entry points, I kept recursing into calls for each homes comps. An important assumption here is that Zillow’s definition of similarity between houses includes location proximity in addition to other factors. Without location proximity, the comp-based traversal of homes will be very non-smooth with respect to location.\nSo, what algorithms are at our disposal for traversing through a network of nodes in different ways? Of course, breadth-first search (BFS) and depth-first search (DFS) quickly come to mind. For the curious, have a look at the basic logic flow of it below. Besides a set membership guard, new homes are only added to the collection when they satisfy the constraints asserted in the meets_criteria function. For now, I do a simple L2 distance check between a pre-defined root lat/long location and the current home’s location. This criterion encouraged the search to stay local to the root, for the purposes of a well-connected and granular heat map. The implementation below uses DFS by popping off the end of the list (line 5) and adding to the end of the list (14), but BFS can be quickly achieved by changing either line (but not both) to instead use the front of the list.\nLetting this algorithm run for 10,000 iterations on Atlanta homes produces the following map in just a few minutes! What’s more, the generated webpage from folium is interactive, allowing common map navigation tools like zooming and panning. To prove out its modularity, I generated some smaller-scale maps of prices for Boston, MA and Seattle, WA as well.\nHeat map of Atlanta housing prices. See the interactive version here.  The Code As promised, here’s the project. It has a Make+Docker setup for ease of use and reproducibility. If you’d like to get an intro to how these two tools come together nicely for reproducible data science, keep reading here. Either way, the README will get you up and running in no time, either via script or Jupyter notebook. Happy viz!\nWhat Now? There are numerous different directions in which we could take this logic next. I’ve detailed a few below for stimulation, but I’d prefer to move in the direction that has the most support, impact, and collaboration. What do you think?\n","date":1563560973,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563560973,"objectID":"a220d87b447b24ba8f9e12b7a03ad01f","permalink":"https://anthonyagnone.com/visualizing-house-price-distributions/","publishdate":"2019-07-19T18:29:33Z","relpermalink":"/visualizing-house-price-distributions/","section":"post","summary":"With Zillow and python's Folium, it's easier than ever","tags":["algorithms","api","data-structures","data-visualization"],"title":"Visualizing House Price Distributions","type":"post"},{"authors":["anthonyagnone"],"categories":["data"],"content":" Motivation When performing experiments in data science and machine learning, two main blockers of initial progress are delays building/using \u0026#8220;base code\u0026#8221; and lack of reproducibility. Thanks to some great open source tools, you don\u0026#8217;t have to be a software guru to circumvent these obstacles and get meaning from your data in a much smoother process.\n\u0026#8220;Hey there, I got this error when I ran your code\u0026#8230;can you help me?\u0026#8221; oh yeah, that file\u0026#8230;  \u0026#8230;and it\u0026#8217;s something facepalm-worthy. Here you are, trying to hit the ground running with a friend or colleague on an interesting idea, and you\u0026#8217;re now side-tracked debugging a file-not-found error. Welcome back to your intro programming course!\nI\u0026#8217;m sure the owner of the code also loves nothing more than to spend a bunch of time helping someone step through these issues at a snail\u0026#8217;s pace. The sheer euphoria you two have just shared over the promise of recent experimental results has now morphed into unspoken embarrassment and frustration that the demonstration has failed before showing any worth, whatsoever.\n  But it\u0026#8217;s fine. It\u0026#8217;s fine! Your buddy knows just where to find that missing file. You\u0026#8217;re told that you will have it within minutes, and then you will be on your way!\n\u0026#8220;Alright, download that file \u0026#8212; I just emailed it to you. Then run train.py, you should get 98% accuracy in 20 epochs.\u0026#8221; Aha! This is it! The time has come to join the ranks of esteemed data magicians, casting one keyboard spell after another, watching your data baby\u0026#8217;s brain get progressively more advanced as it beckons for a role in a new Terminator movie! Let\u0026#8217;s see what we get!\nbut I did what you said 🙁  \u0026#8230;yeah, we\u0026#8217;ve all been there.\nWhat could it be? Well, maybe it\u0026#8217;s something obvious. I know python, and I know what your code should be doing. I\u0026#8217;ll just pop open your train.py to poke around and\u0026#8230;NOPE.\n  Don\u0026#8217;t worry, this isn\u0026#8217;t going to be a pinky-waving article about how to always write a software masterpiece and scoff at anything you deem insubordinate. That\u0026#8217;s a sticky subject in general, as it\u0026#8217;s wrought with subjectivity and competing standards. These examples aim to just emphasize how there are a myriad of ways in which we would not prefer for new experiments to start.\nWe\u0026#8217;re interested in re-producing and improving on results in a convenient fashion, not stumbling to re-create past achievements. With that in mind, let\u0026#8217;s have a look at some popular tools that can be used to streamline the start of any new ML software project: Docker and Make.\nDocker The python ecosystem has some great features for dealing with dependencies, such as pip and virtualenv. These tools allow for one to easily get up and running according to some specification of what needs to be installed to proceed with running some code.\nFor example, say you have just come across the scikit-learn library (and it\u0026#8217;s love at first sight, of course). You are particularly drawn to one of its demo examples, but would like to re-produce it with the data housed in a pandas DataFrame. Furthermore, another project you are working on requires an ancient version of pandas, but you would like to use features available only in a newer version. With pip and virtualenv, you have nothing to fear (\u0026#8230;but fear itself).\n# create and activate environment virtualenv pandas_like_ml source pandas_like_ml/bin/activate # install your desired libraries pip install --upgrade pip pip install scikit-learn==0.21.1 pip install pandas==0.19.1 # the main event python eigenfaces.py -n 20000 # we're done here, so exit the environment source deactivate  When you learn this flow for the first time, you feel freed from the hellish existence that is dependency management. You triumphantly declare that you shall never, ever be conquered again by the wrath of a missing package or a bloated monolithic system environment. However, this unfortunately isn\u0026#8217;t always enough\u0026#8230;\n Python environment tools fall short when the dependency is not at the language level, but at the system level.  For example, say you would like to set up your machine learning project with a MongoDB database backend. No problem! pip install pymongo and then we\u0026#8217;re home free! Not so fast\u0026#8230;\n  Well\u0026#8230;that didn\u0026#8217;t go as expected. Now, in addition to setting up my library dependencies, we need to also manage a library outside of python? Gah! Further delays! Time to google for the package name for mongoDB\u0026#8230;\nWhat if I don\u0026#8217;t even know what operating system my colleague is using? I can\u0026#8217;t give him some sudo apt-get install snippet if he\u0026#8217;s on CentOS. Even more to the point, there\u0026#8217;s no easy way to automate this step for future projects. Make me do something once, I\u0026#8217;ll do it. Make me do it again\u0026#8230;zzzz.\nSo, we\u0026#8217;re faced with the desire to standardize and automate setting up software libraries and other system dependencies for new data-related endeavors, and sadly our usual python tools have fallen short. Enter Docker: an engine for running services on an OS as lightweight virtualization packages called containers. Docker containers are the realization of the definition of a Docker image, which is specified by a file called a Dockerfile.\n# you can specify a base image as a foundation to build on FROM ubuntu:16.04 # make a partition, and specify the working directory VOLUME /opt WORKDIR /opt # install some base system packages RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ python3 \\ python3-dev \\ python3-pip \\ python3-setuptools # install some python packages RUN pip3 install --upgrade pip RUN pip3 install \\ scikit-learn==0.21.1 \\ pandas==0.19.1 # set the container's entry point, just a bash shell for now. # this can also be a single program to run, i.e. a python script. ENTRYPOINT [\u0026quot;/bin/bash\u0026quot;]  Think of a Dockerfile as a (detailed) recipe of setup steps we would need to do in order to get the system in the state we would like for the experiment. Examples include things like setting up a database, installing libraries, and initializing a directory structure. If you\u0026#8217;ve ever made a nice shell script to do some setup like this for you, you were not far from the typical Docker workflow. There are many benefits that Docker has over a shell script for this, most notably being containerization: with Docker containers, we are abstracted away from the host system that the container is running on. The virtual system that the container is running in is defined in its own process. Because of this, we can have multiple containers running completely different setups, but on the same host machine. How\u0026#8217;s that for some insulation against system dependency hell?\nAdditionally, we are further insulated from issues like missing files and differences of system state. We know exactly what the system state will be when it is run. We know this because we have made it so via the explicit instructions in the Dockerfile.\nTo actually build the image, we use a command like the following:\ndocker build \\ -t my_first_container \\ -f Dockerfile  At this point, we have built the image. With this image, we can repeatedly instantiate it as desired, e.g. to perform multiple experiments.\ndocker run \\ --rm \\ -it \\ my_first_container  Voila!\nIf we left at this point and ran in N directions to do various different experiments, these commands may get rather cumbersome to type\u0026#8230;\ndocker run \\ --mount type=bind,source=\u0026quot;$(pwd)\u0026quot;,target=/opt \\ --mount type=bind,source=${CORPORA_DIR},target=/corpora \\ -p ${JUPYTER_PORT}:${JUPYTER_PORT} \\ -ti \\ --rm \\ my_advanced_container \\ jupyter-lab \\ --allow-root \\ --ip=0.0.0.0 \\ --port=${JUPYTER_PORT} \\ --no-browser \\ 2\u0026gt;\u0026amp;1 | tee log.txt  Don\u0026#8217;t worry if your eyes gloss over at this. The point is it\u0026#8217;s a lot to keep typing. That\u0026#8217;s fine though, we have shell scripts for a reason. With shell scripts, we can encapsulate minute details of making a very specific sequence of commands into something as mindless as bash doit.sh. However, consider also a scenario in which your Dockerfile definition depends on other files (i.e. a requirements.txt file or a file of environment variables to use). In this case, we also would like to know automatically when the Docker image needs to be re-created, based on upstream dependencies.\nSo what has four letters, saves you from typing long, arduous commands, and automates dependency management?\nMake GNU Make is a wonderous tool, gifted to us by the same software movement that has made the digital world what it is today. I\u0026#8217;ll save you a more sparkly introduction and jump into the core abstraction of what it is: a DAG-based approach to intelligently managing dependencies of actions in a process, in order to efficiently achieve a desired outcome.\nOk, it\u0026#8217;s also a convenient way to compile C code. But focus on the first definition, and think bigger! Re-using the general DAG-based dependency management idea has led to some great tools over the years, like Drake (not the rapper), Luigi (not Mario\u0026#8217;s brother), and perhaps most notably Airflow (AirBnB\u0026#8217;s baby, but now part of the Apache Foundation).\nConsider the contrived example below. We\u0026#8217;d like to make predictions on audio-visual data with a trained model. As a new raw image appears, do we need to re-train the model in order to create a prediction? Setting aside applications such as online learning, we do not. Similarly, say we just updated some parameters of our trained model. Do we need to re-cull the raw images, in order to re-create the same data sample? Nope.\n  This is where Make comes into play. By specifying a Makefile with \u0026#8220;targets\u0026#8221; that correspond to (one or more) desired outputs in the DAG, invoking that target will automatically provide that outcome for you, while only re-invoking dependency processes that are necessary.\nMake can be used for pretty much anything that involves actions and their dependencies. It\u0026#8217;s not always right tool in the shed (see Airflow for this process on distributed applications), but it can get you pretty far. I even used it to generate the image above! Here\u0026#8217;s what the Makefile looks like.\n# the \u0026quot;graph.png\u0026quot; target specifies \u0026quot;graph.dot\u0026quot; as a dependency # when \u0026quot;graph.png\u0026quot; is invoked, it invokes \u0026quot;graph.dot\u0026quot; only if necessary graph.png: graph.dot dot graph.dot -Tpng \u0026gt; graph.png # the \u0026quot;graph.dot\u0026quot; target specifies \u0026quot;make_graph.py\u0026quot; as a dependency # so, this command is only re-run when... # 1) make_graph.py changes # 2) graph.dot is not present graph.dot: make_graph.py python make_graph.py  Marrying the Two So we\u0026#8217;ve ailed over to struggles of reproducible work and introduced great tools to manage environment encapsulation (Docker) and dependency management (Make). These are two pretty cool cats, we should introduce them to each other!Photo by\u0026nbsp;Product School\u0026nbsp;on\u0026nbsp;Unsplash\nP.S. Which one is Docker, and which is Make?\nLet\u0026#8217;s say we\u0026#8217;ve just found the Magenta project, and would like to set up an environment to consistently run demos and experiments in, without further regard to what version of this_or_that.py is running on someone\u0026#8217;s computer. After all, on some level, we don\u0026#8217;t care what version of this_or_that.py is running on your machine. What we care is that you are able to experience the same demo/result that the sender has experienced, with minimal effort.\nSo, let\u0026#8217;s set up a basic Dockerfile definition that can accomplish this. Thankfully, the Magenta folks have done the due diligence of creating a base Docker image themselves, to make it trivial to build from:\n# base image FROM tensorflow/magenta # set partition and working directory VOLUME /opt WORKDIR /opt # install base system packages RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ vim \\ portaudio19-dev # install python libraries COPY requirements.txt /tmp/requirements.txt RUN pip install --upgrade pip RUN pip install -r /tmp/requirements.txt # container entry point ENTRYPOINT [\u0026quot;/bin/bash\u0026quot;]  After specifying the base image as Magenta\u0026#8217;s, we set a working directory on an /opt volume, install some system-level and python-level dependencies, and make a simple bash entry point until we have a working application. A typical requirements.txt file might look like this:\njupyterlab seaborn scikit-learn matplotlib pyaudio  Awesome. So now we have a specification of our desired environment. We can now make a Makefile which handles some of the dependencies at play:\n# use the name of the current directory as the docker image tag DOCKERFILE ?= Dockerfile DOCKER_TAG ?= $(shell echo ${PWD} | rev | cut -d/ -f1 | rev) DOCKER_IMAGE = ${DOCKER_USERNAME}/${DOCKER_REPO}:${DOCKER_TAG} $(DOCKERFILE): requirements.txt docker build \\ -t ${DOCKER_IMAGE} \\ -f ${DOCKERFILE} \\ . .PHONY image image: $(DOCKERFILE) .PHONY: run run: nvidia-docker run \\ --mount type=bind,source=\u0026quot;$(shell pwd)\u0026quot;,target=/opt \\ -i \\ --rm \\ -t $(DOCKER_IMAGE)  This Makefile specifies targets for run, image, and $(DOCKERFILE). The $(DOCKERFILE) target lists requirements.txt as a dependency, and thus will trigger a re-build of the Docker image when that file changes. The image target is a simple alias for the $(DOCKERFILE) target. Finally, the run target allows a concise call to execute the desired program in the Docker container, as opposed to typing out the laborious command each time.\nOne Docker to Rule Them All?   At this point, you may be motivated to go off and define every possible dependency in a Dockerfile, in order to never again be plagued with the troubles of ensuring an appropriate environment for your next project. For example, Floydhub has an all-in-one Docker image for deep learning projects. This image specification includes numerous deep learning frameworks and supporting python libraries.\nDon\u0026#8217;t do that!\nFor the sake of argument, let\u0026#8217;s take that to the limit. After the next 100 projects that you work on, what will your Docker image look like? And what about after the next 1000 projects? Over time, it will just become as bloated as if you had incrementally changed your main OS in each project. This goes against the containerization philosophy of Docker \u0026#8212; your containers should be lightweight while remaining sufficient.\nFurthermore, with all of that bloat you lose the ability to sustain multiple directions of projects that require different versions of dependencies. What if one of your projects requires the latest version of Tensorflow to run, but you don\u0026#8217;t want to update the 99 previous projects (and deal with all of the failures the updates bring)?\nConclusion In this part of the Towards Efficient and Reproducible (TEAR) ML Workflows series, we\u0026#8217;ve established the basis for making experiments and applications a relatively painless process. We used containerization via Docker to ensure experiments and applications are reproducible and easy to execute. We then used some automatic dependency management via Make for keeping experiment pipelines efficient and simple to run.Photo by\u0026nbsp;Susan Holt Simpson\u0026nbsp;on\u0026nbsp;Unsplash\nIt\u0026#8217;s worth noting that there are numerous alternative solutions to these two; however, they follow the same general patterns: containerization gives you reproducibility and automatic dependency management gives you efficiency. From there, the value added in other solutions usually comes down to bells and whistles like cloud integration, scalability, or general ease of use. To each, your own choice of tools.\nNext, we’ll look at giving some more power to each of these processes, saving us more time and making them more reusable. We’ll also look at some best practices on how to properly collaborate with others when building these processes.\n","date":1562782261,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562782261,"objectID":"9f08d1e7431a50b8f78468f8d8b373a9","permalink":"https://anthonyagnone.com/reproducible-data-processing-make-docker/","publishdate":"2019-07-10T18:11:01Z","relpermalink":"/reproducible-data-processing-make-docker/","section":"post","summary":"Avoiding reproducibility hell with dependency management and containerization.","tags":["machine-learning","reproducability","software"],"title":"Reproducible Data Processing: Make + Docker","type":"post"},{"authors":["anthonyagnone"],"categories":["musing"],"content":" I like information. After all, it\u0026#8217;s the removal of uncertainty from our minds. And boy, do we hate uncertainty. We just want to know it!\nIt\u0026#8230;? What is it? It is anything\u0026#8230;and it is everything. No partially this or partially that. We just want to know, one way or the other.\nWe are now creating more information each second than it had taken thousands (and billions, depending on who \u0026#8220;we\u0026#8221; refers to) of years to previously create [1][2].\nBut damn\u0026#8230;isn\u0026#8217;t it just plain hard nowadays to manage it all?\nA lot of you will have already clicked on the various links in the previous sentences, and maybe even made sure to open them in a new browser tab, so as to not take you away from this page. That implies you love organizing information too. But how do we manage all of that information in an age where it is exponentially more prevalent than we’ve ever had to deal with before? This prevalence is both a gift and a curse.\nA Gift It\u0026#8217;s a gift because we now are able to resolve uncertainty at unprecedented levels. A world of information is literally at our fingertips, with the relatively negligible (all things considered, at least) weight of a computer or mobile phone. On any given day, you can decide to learn about almost any subject, and likely also do so without much, if any, financial loss. This kind of luxury was only a pipe dream for previous generations, where obtaining a new skill likely meant massive financial commitments, re-location, and lugging around several pounds worth of textbooks. For what it\u0026#8217;s worth, universities aren\u0026#8217;t completely unlike this today; they are, however, similar to a much smaller extent.\nA Curse But slow down for a moment. There are also some severe negatives to these rapid changes in the world. This unprecedented level of inbound information flow has also led to unprecedented amounts of stress on our brains, due to all the uncertainty introduced. The universe is a closed system, with finite resources to drive change in the form of evolution. However, it\u0026#8217;s plausible to say that our technological progress has (at least, to date) been advancing far more rapidly than, our biological selves can.\nTake a moment to close your eyes. Think about the last time you were in a place completely removed from anything digital. Maybe it was a walk on the beach during a beautiful orange sunset. Maybe it was a cool, morning hike on your favorite mountain trail with your best friend. Bask in the memory of how quiet, peaceful, and equal-tempered your surroundings were.\nNow\u0026#8230;let your attention rush back into the present, and you\u0026#8217;ve likely found that countless things have already sucked you back in. Televisions, phones, computer displays, car horns, billboards, etc.\nThese things pack a ton of information in them, and a lot of them do so very explicitly. But this information doesn\u0026#8217;t just resolve uncertainty; it also creates\u0026nbsp;more, and often more than it had resolved in the first place. Take, for example, checking your phone. You hit a wake button and your are flooded with work emails, pictures from your friends, questions from your parents, voicemails from recruiters, voicemails from spam, attempted fraud on your bank account, etc. Through this information, you\u0026#8217;ve just resolved the uncertainties of whether those events have occurred or not. However, they\u0026#8217;ve also seized your attention for the foreseeable minutes of your life to resolve the additional uncertainties they have created. Obviously, some of these notifications are leaps and bounds more useful than others (protecting your bank account vs. laughing at a meme). However, we need to learn as humans and as societies of humans how to more effectively manage this explosion of information to retain our sanity.\nThere\u0026#8217;s another factor of information overload at play here: others profiting from controlling your attention. With the spread of free knowledge and software, a leading way to profit in platforms such as mobile apps and news media is advertising. This strategy provides information to you for free, but profits from allowing other entities to come along for the right and try to whisk you away with a quick \u0026#8220;Hey, look at me! Click on me! Make me money.\u0026#8221; These methods even are designed to optimally take and keep your attention, since it\u0026#8217;s how the success of the designers\u0026#8217; methods is measured.\nThink about that: someone else prospers when they can keep you focusing on what they want you to be focusing on, even if it causes record-breaking amounts of stress for you. Granted, they likely aren\u0026#8217;t viewing it like that; they\u0026#8217;re just trying to make money to help their own efforts and to get by in life. However, if you allow the thousands of designed attention-grabbers to jerk your focus one way or another constantly, you end up just a spiraling pawn in their attention game.\nThe Good Fight Now, we\u0026#8217;re fighting to take control back. We feel a little less human when we find ourselves being tugged back and forth between a focus on this and that. We\u0026#8217;re talk about attention as an ever-more scarce\u0026nbsp;resource. We\u0026#8217;re framing all this digital information as being sometimes too much to efficiently process, and therefore something that we need to detox from. But who is to blame for all of this? The designers and marketers? The politicians? The laws of physics? Ourselves?\nPerspective I\u0026#8217;m not interested in blame in this setting. I\u0026#8217;m interested in taking control where I can, and re-achieving the daily sense of calm I used to easily have before the whirlwind of digital information reached me. One control we will hopefully always have is what we choose to allow to reach our senses, and therefore divert our attention. I\u0026#8217;m not going to blow this article up into a to-do list for iPhone notification settings. What I will way though, is that you have a lot of capability to reduce the distracting signals in your everyday environment, whether it\u0026#8217;s software settings, work environment setup, or home-life setup. What I think you\u0026#8217;ll find is that the more you chip away at things like constant notifications, sounds, and attention shifts, the more you\u0026#8217;ll get back in touch with that endorphin-inducing sense of focus and serenity that leads to a happier and more meaningful day.\nI\u0026#8217;m interested in anybody\u0026#8217;s time-proven methods for achieving this goals. Please share!\n","date":1550257289,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550257289,"objectID":"33342d992fb919439dc07e8d471435f2","permalink":"https://anthonyagnone.com/the-war-on-attention/","publishdate":"2019-02-15T19:01:29Z","relpermalink":"/the-war-on-attention/","section":"post","summary":"Thoughts on information and self-control in a modern attention economy","tags":["attention","information","management","notification","stress","time"],"title":"The War on Attention","type":"post"}]